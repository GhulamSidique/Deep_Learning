{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing:\n",
    "### it is a method to perform some operations on an image in order to get an enhanced image or to extract useful information from it. it is a type of a signal processing in which input is an image and output maybe an image or characterfistics associated with that image.\n",
    "#### -----------------------------------------------------------------------------------------\n",
    "#### -----------------------------------------------------------------------------------------\n",
    "## Computer Vision:\n",
    "### it is a field of computer science that works on enabling computers to see, identify and process images in the same way that human vision does and then provide appropriate ouptut.\n",
    "#### -----------------------------------------------------------------------------------------\n",
    "#### -----------------------------------------------------------------------------------------\n",
    "## CNN:\n",
    "### these use 3D data for image classification and objet recognition tasks. \n",
    "### it is made up of multiple layers like convolutional layer, pooling layer, flattenning and fully connected layers.\n",
    "## 1) Convolutional layers: \n",
    "### these layers apply convolutional operations on the input images, using filters(kernels) to detect features such as edges, textures, and more complex patterns.\n",
    "### original image is converted into feature map(compressed) doing this will lose the less strength in the dataset. multiple feature maps are made from an image.  \n",
    "## 2) Pooling layers:\n",
    "### Pooling layers downsample the spatial dimensions of the input, reducing the computational complexity and the number of parameters in the network. Max pooling is a common pooling operation, selecting the maximum value from a group of neighboring pixels.\n",
    "### it operates on each feature map independently and reduces the feature maps and makae use of the max pooling by selecting the max value and put it in the pooled feature map.\n",
    "### we can also make us eof average pooling.\n",
    "## 3) Flattening:\n",
    "### it is used to convert the data into 1D array for inputting it to the next layer to create a signle long feature vector and it is connected to the final classification model called a fully connectted layer.\n",
    "## 4) Fully Connected Layers:\n",
    "### These layers are responsible for making predictions based on the high-level features learned by the previous layers. They connect every neuron in one layer to every neuron in the next layer.\n",
    "#### -----------------------------------------------------------------------------------------\n",
    "#### -----------------------------------------------------------------------------------------\n",
    "## Activation function(Softmax):\n",
    "### we make use of the softmax function here that takes as input a vector of K real numbers and normalizes it into a probability distribution consisting of K probabalities proportional to the exponentials of the input numbers. \n",
    "### formula --> fj(z) = (e)**zj / sum((e)**zk)\n",
    "#### -----------------------------------------------------------------------------------------\n",
    "#### -----------------------------------------------------------------------------------------\n",
    "## Cross_entropy:\n",
    "### it is a masure from the field of information theory building upon entropy and generally calculating the difference between two probability distributions\n",
    "### formula --> L = -log((e)**f / sum(e)**f)\n",
    "### formula --> H(p,q) = -sum( p(x) logq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporing  libraries0\n",
    "from keras.models import Sequential\n",
    "# here conv2d means convolutional layer with 2d\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# for implemnting cnn we need to implement the following steps\n",
    "\n",
    "# 1. Initializing cnn\n",
    "classifier=Sequential()\n",
    "\n",
    "# 2. 1st convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape= (64, 64, 3), activation='relu'))  \n",
    "# 32, the number of filters (also known as kernels) in the convolutional layer. Each filter will learn to recognize different features in the input image. Having 32 filters means the layer will output 32 different feature maps.\n",
    "# (3, 3) This is the size of the filters (height and width). In this case, each filter is 3x3 pixels. The filter size determines the area of the input that the filter will \"look at\" when creating the output feature map.\n",
    "# (64, 64, 3) the input to this layer is expected to be images with a height of 64 pixels, a width of 64 pixels, and 3 color channels (typically corresponding to the RGB channels).\n",
    "\n",
    "# 3. Second conv layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "# 4. Maxpooling\n",
    "classifier.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# The Max Pooling operation will look at each 2x2 block of the input feature map and take the maximum value from that block.\n",
    "# in Conv2d we have 64*64 pixel picture and here the Ma\\xpool of size 2*2 will shrink it to 32*32 pixel\n",
    "\n",
    "# 5. flattening\n",
    "classifier.add(Flatten())\n",
    "# this will convert the neural network into a 1d array\n",
    "\n",
    "# 6. establishing the full connection \n",
    "# input layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "# output layer\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# 7. Compile the neural network\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an image generator for our model\n",
    "# for this we need to import the image data generator\n",
    "# ImageDataGenerator is used to generate batches of tensor image data with real-time data augmentation.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# training data generator\n",
    "train_data_gen = ImageDataGenerator(rescale = 1./255, shear_range= 0.2, zoom_range = 0.2)\n",
    "# 1./255, dividing by 255 normalizes these values to a range of 0 to 1\n",
    "# The shear_range value of 0.2 means that the image could be sheared up to an angle that corresponds to a shear intensity of 0.2 radians.\n",
    "# The value 0.2 means that the image can be zoomed in or out by up to 20% randomly. For example, if you shear a rectangle, it might turn into a parallelogram, where the top of the shape is shifted relative to the bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# training dataset from the directory\n",
    "training_d_set=train_data_gen.flow_from_directory('E:/complete ML/DL/L#08_CNN/dataset/training_set', target_size=(64,64), batch_size=32,\n",
    "    class_mode='binary')\n",
    "# The flow_from_directory method is a function provided by the ImageDataGenerator class. It is used to load images directly from a directory on your disk and generate batches of augmented data for training.\n",
    "# all images are resized to 64x64 pixels\n",
    "# The batch_size parameter determines how many images are returned in each batch. Here, 32 images will be returned in each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result: the above output means we have total 8000 images in our 2 subdirectories i.e cat and dog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# doing same as the above for our test data set\n",
    "test_d_set= train_data_gen.flow_from_directory('E:/complete ML/DL/L#08_CNN/dataset/test_set', target_size=(64,64), batch_size=32,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result: from result we have 2000 images for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m 250/8000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m52:49\u001b[0m 409ms/step - accuracy: 0.5413 - loss: 0.7858"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 17ms/step - accuracy: 0.5810 - loss: 0.6944 - val_accuracy: 0.6220 - val_loss: 0.6420\n",
      "Epoch 2/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6ms/step - accuracy: 0.6811 - loss: 0.5990 - val_accuracy: 0.7225 - val_loss: 0.5647\n",
      "Epoch 3/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6ms/step - accuracy: 0.7280 - loss: 0.5450 - val_accuracy: 0.7290 - val_loss: 0.5473\n",
      "Epoch 4/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 7ms/step - accuracy: 0.7461 - loss: 0.5111 - val_accuracy: 0.7355 - val_loss: 0.5355\n",
      "Epoch 5/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6ms/step - accuracy: 0.7692 - loss: 0.4770 - val_accuracy: 0.7530 - val_loss: 0.5044\n",
      "Epoch 6/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 7ms/step - accuracy: 0.7852 - loss: 0.4551 - val_accuracy: 0.7335 - val_loss: 0.5562\n",
      "Epoch 7/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 7ms/step - accuracy: 0.7944 - loss: 0.4320 - val_accuracy: 0.7655 - val_loss: 0.5036\n",
      "Epoch 8/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 7ms/step - accuracy: 0.8114 - loss: 0.4137 - val_accuracy: 0.7750 - val_loss: 0.4947\n",
      "Epoch 9/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 7ms/step - accuracy: 0.8183 - loss: 0.4006 - val_accuracy: 0.7690 - val_loss: 0.5187\n",
      "Epoch 10/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 7ms/step - accuracy: 0.8187 - loss: 0.3909 - val_accuracy: 0.7620 - val_loss: 0.5334\n",
      "Epoch 11/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 7ms/step - accuracy: 0.8335 - loss: 0.3664 - val_accuracy: 0.7690 - val_loss: 0.5136\n",
      "Epoch 12/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 6ms/step - accuracy: 0.8475 - loss: 0.3455 - val_accuracy: 0.7660 - val_loss: 0.5516\n",
      "Epoch 13/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 6ms/step - accuracy: 0.8506 - loss: 0.3355 - val_accuracy: 0.7765 - val_loss: 0.5105\n",
      "Epoch 14/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 6ms/step - accuracy: 0.8623 - loss: 0.3156 - val_accuracy: 0.7760 - val_loss: 0.5388\n",
      "Epoch 15/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 6ms/step - accuracy: 0.8662 - loss: 0.3120 - val_accuracy: 0.7580 - val_loss: 0.5461\n",
      "Epoch 16/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 7ms/step - accuracy: 0.8792 - loss: 0.2801 - val_accuracy: 0.7685 - val_loss: 0.5554\n",
      "Epoch 17/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 6ms/step - accuracy: 0.8844 - loss: 0.2738 - val_accuracy: 0.7875 - val_loss: 0.5559\n",
      "Epoch 18/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - accuracy: 0.8912 - loss: 0.2532 - val_accuracy: 0.7715 - val_loss: 0.6114\n",
      "Epoch 19/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6ms/step - accuracy: 0.9018 - loss: 0.2338 - val_accuracy: 0.7750 - val_loss: 0.5874\n",
      "Epoch 20/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 6ms/step - accuracy: 0.9050 - loss: 0.2304 - val_accuracy: 0.7715 - val_loss: 0.6106\n",
      "Epoch 21/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 6ms/step - accuracy: 0.9162 - loss: 0.2110 - val_accuracy: 0.7505 - val_loss: 0.6371\n",
      "Epoch 22/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 6ms/step - accuracy: 0.9163 - loss: 0.1992 - val_accuracy: 0.7690 - val_loss: 0.6714\n",
      "Epoch 23/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6ms/step - accuracy: 0.9194 - loss: 0.1982 - val_accuracy: 0.7610 - val_loss: 0.7083\n",
      "Epoch 24/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - accuracy: 0.9361 - loss: 0.1586 - val_accuracy: 0.7685 - val_loss: 0.7471\n",
      "Epoch 25/25\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6ms/step - accuracy: 0.9297 - loss: 0.1746 - val_accuracy: 0.7570 - val_loss: 0.7478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25225eacd10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will fit the training dataset into our model as follows\n",
    "classifier.fit(training_d_set, steps_per_epoch = 8000, epochs = 25, validation_data= test_d_set, validation_steps= 2000)\n",
    "# fit_generator is a method used to train a model on data provided by a generator\n",
    "# steps_per_epoch = 8000, for each epoch, the model will process 8000 batches of data from the training_d_set.\n",
    "# epochs = 25, This parameter specifies the number of times the model will go through the entire training dataset.\n",
    "# The validation_data parameter specifies that this generator will be used to evaluate the model’s performance on validation data after each epoch.\n",
    "# validation_steps= 2000, This parameter specifies the number of batches to draw from the test_d_set generator for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "It is a Dog\n"
     ]
    }
   ],
   "source": [
    "# no making predictions as follows\n",
    "from keras.preprocessing import image # will be used to load the image\n",
    "# we will also make use of numpy to convert image into an array\n",
    "import numpy as np\n",
    "\n",
    "# load the test image \n",
    "test_image=image.load_img(\"E:/complete ML/DL/L#08_CNN/dataset/single_prediction/cat_or_dog_1.jpg\", target_size=(64,64))\n",
    "# here the loaded image is of \"dog\" so we will expect our model to predict the Dog\n",
    "# cat_or_dog_2.jpg => this image is of cat so the model must predict cat\n",
    "\n",
    "# convert the image into an array\n",
    "test_image=image.img_to_array(test_image)\n",
    "\n",
    "test_image=np.expand_dims(test_image, axis=0)\n",
    "#  By adding a new axis at position 0, it changes the shape of test_image from (64, 64, 3) to (1, 64, 64, 3). This is done because the model expects a batch of images as input, even if you're only predicting for a single image. The added dimension represents the batch size.\n",
    "\n",
    "# The predict method returns the probability of each class (e.g., dog or cat) as an array. \n",
    "result = classifier.predict(test_image)\n",
    "\n",
    "training_d_set.class_indices\n",
    "# This dictionary maps the class labels (e.g., {'cats': 0, 'dogs': 1}) to their corresponding indices used in training. This information helps interpret the model's output.\n",
    "\n",
    "# conditions to check the predictions\n",
    "if result[0][0] ==1:\n",
    "    prediction = 'It is a Dog'\n",
    "\n",
    "else:\n",
    "    prediction ='It is a Cat'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
