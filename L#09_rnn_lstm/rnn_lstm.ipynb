{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (Recurrent Neural Network) is a type of neural network designed to handle sequential data by maintaining a \"memory\" of previous inputs. This makes RNNs particularly useful for tasks where the order or context of the data is important, such as in time series forecasting, natural language processing (NLP), speech recognition, and machine translation.\n",
    "### all nlp problems can make use of rnn like translation of one language to another. another example is that suppose we have a sentence containing name osf the person, company's name and year, if we want to separate these as different fields like person's name, company's name and year then it can be achieved by making use of the rnn.\n",
    "### it can also be used in sentiments analysis.\n",
    "### sequence is necessary here. \n",
    "## issues with rnn:\n",
    "### 1) suppose that our model is trained on the \"how are you\" text and we have another input text with some extension in text like \"how are you and your brother\", then rnn might make a problem here.\n",
    "### 2) too much ccomputation is required here.\n",
    "\n",
    "### Vanishing gradient problem in rnn: It occurs when gradients used to update the weights during backpropagation become extremely small, effectively preventing the network from learning.\n",
    "\n",
    "### Types of RNN Architectures:\n",
    "### 1) Many-to-Many: This is useful for tasks where both the input and output are sequences, such as in language translation (sequence of words as input, sequence of words as output).\n",
    "### 2) Many-to-One: In this configuration, the RNN processes a sequence of inputs and produces a single output at the end, such as in sentiment analysis (sequence of words as input, sentiment as output).\n",
    "### 3) One-to-Many: Here, a single input produces a sequence of outputs, like in music generation (a single theme producing a sequence of notes).\n",
    "### 4) One-to-One: This is a simple case where both input and output are individual elements, but this isn't a common use of RNNs.\n",
    "\n",
    "## LSTM: Long short term memory is a type of recurrent neural network (RNN) architecture designed to handle long-term dependencies in sequential data. It was introduced to address the shortcomings of standard RNNs, particularly the vanishing gradient problem, which makes it difficult for RNNs to learn and remember information over long sequences.\n",
    "### Key Concepts of LSTM:\n",
    "### 1) Memory Cells: LSTMs introduce memory cells that are responsible for maintaining information over long sequences. These cells have the ability to decide when to keep, update, or forget information through specialized gates.\n",
    "### 2) Gates: The central innovation in LSTMs is the use of three types of gates that regulate the flow of information through the memory cell:\n",
    "#### a) Forget Gate: Decides which information from the previous time step should be forgotten.\n",
    "#### b) Input Gate: Determines what new information should be added to the memory cell.\n",
    "#### c) Output Gate: Controls what information from the memory cell should be output at the current time step.\n",
    "### 3) Cell State: The cell state acts as a conveyor belt that runs through the entire sequence. It allows information to flow through the network unchanged unless explicitly modified by the gates. This helps LSTMs retain long-term dependencies effectively.\n",
    "\n",
    "## Working of lstm\n",
    "### 1) Decides how much of the past it should remember decided by the sigmoid function. \n",
    "### 2) decides how much should this unit add to the current state. in this step we have sigmoid function that decides which values to let through (0 or 1) and other is tanh which gives the wieghtage to the values which are passed deciding their level of importance (-1 or 1).\n",
    "### 3) decides what part of the current cell state makes it to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here we will be working with the stock price predictions dataset to predict the prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# for model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>12/23/2016</td>\n",
       "      <td>790.90</td>\n",
       "      <td>792.74</td>\n",
       "      <td>787.28</td>\n",
       "      <td>789.91</td>\n",
       "      <td>623,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>12/27/2016</td>\n",
       "      <td>790.68</td>\n",
       "      <td>797.86</td>\n",
       "      <td>787.66</td>\n",
       "      <td>791.55</td>\n",
       "      <td>789,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>12/28/2016</td>\n",
       "      <td>793.70</td>\n",
       "      <td>794.23</td>\n",
       "      <td>783.20</td>\n",
       "      <td>785.05</td>\n",
       "      <td>1,153,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>12/29/2016</td>\n",
       "      <td>783.33</td>\n",
       "      <td>785.93</td>\n",
       "      <td>778.92</td>\n",
       "      <td>782.79</td>\n",
       "      <td>744,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>12/30/2016</td>\n",
       "      <td>782.75</td>\n",
       "      <td>782.78</td>\n",
       "      <td>770.41</td>\n",
       "      <td>771.82</td>\n",
       "      <td>1,770,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close      Volume\n",
       "0       1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1       1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2       1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3       1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4       1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
       "...          ...     ...     ...     ...     ...         ...\n",
       "1253  12/23/2016  790.90  792.74  787.28  789.91     623,400\n",
       "1254  12/27/2016  790.68  797.86  787.66  791.55     789,100\n",
       "1255  12/28/2016  793.70  794.23  783.20  785.05   1,153,800\n",
       "1256  12/29/2016  783.33  785.93  778.92  782.79     744,300\n",
       "1257  12/30/2016  782.75  782.78  770.41  771.82   1,770,000\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the tarining dataset\n",
    "t_d_set=pd.read_csv(\"Google_Stock_Price_Train.csv\")\n",
    "t_d_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will work only with the open values for our training purpose\n",
    "training_dset=t_d_set.iloc[:, 1:2].values # here we have specified that we need all the rows and only 1 and 2 columns namely open and high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to scale the data to avoid outliers\n",
    "sc= MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# scale the training dataset\n",
    "scalled_training_set=sc.fit_transform(training_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to separate the x and y train values\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "# we will run our code with 60 time steps till 1258 that is the total size of the dataset, 60 is often taken in the lstm forecasting problems \n",
    "for i in range(60, 1258):\n",
    "    # xtrain will contain values from 0 to 59 (i=60-60:i=60 --> 0 to 59)\n",
    "    X_train.append(scalled_training_set[i-60:i, 0])\n",
    "    # y train will contain (60 when xtrain is 0 to 59, similary if xtarin is 1 to 60 the ytrain will be 61)\n",
    "    y_train.append(scalled_training_set[i, 0])\n",
    "# conevrting xtrain and ytrain into np array\n",
    "X_train, y_train=np.array(X_train), np.array(y_train)\n",
    "\n",
    "# reshaping xtrain\n",
    "# here x_train.reshape[0] ==> if you have 1,198 samples (because your loop runs from 60 to 1258), this value will be 1,198.\n",
    "# and X_train.shape[1]--> each sample will consist of 60 consecutive time points from the dataset.\n",
    "# and 1 meams we have made use of only one input sample that is Open balance\n",
    "X_train=np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation\n",
    "\n",
    "# 1) initialize the model\n",
    "sq=Sequential()\n",
    "\n",
    "# 2) adding the lstm \n",
    "# with input layer\n",
    "# here 50 units means that our lstm will have 50 memory cells\n",
    "# When return_sequences=True, the LSTM layer will return the full sequence of outputs for each time step.\n",
    "# Dropout randomly \"drops\" (i.e., ignores) 20% (0.2) of the neurons during training on each forward pass. This forces the model to become more robust, as it cannot rely on any one particular neuron during training.\n",
    "# No need to specify an additional activation function for the LSTM layer: The LSTM already uses the sigmoid and tanh activation functions internally.\n",
    "sq.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "sq.add(Dropout(0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
